{% extends "base.html" %}
{% block title %}Model Overview{% endblock %}
{% block content %}
<section class="model-hero">
    <span class="eyebrow">Model Intelligence</span>
    <h1>What powers each prediction</h1>
    <p>Cohesive explanations for every model let you see which inputs, layers, and hyperparameters influence <br>the outcome before you upload an image or type clinical numbers.</p>
    <div class="cta-row">
        <a class="btn" href="{{ url_for('home') }}">Back to dashboard</a>
        <a class="btn secondary" href="{{ url_for('brain_tumor') }}">Try brain tumor scan</a>
    </div>
</section>

<section class="model-grid">
    <article class="model-card">
        <div>
            <span class="badge">CNN</span>
            <h3>Brain Tumor Classifier</h3>
            <p class="muted">Classifies MRI slices into glioma, meningioma, pituitary, or healthy tissue using a custom convolutional stack trained on the Brain Tumor Classification dataset.</p>
        </div>
        <div class="stat-row">
            <div class="stat-block">Input: 150×150×3 RGB (converted to BGR before Keras)</div>
            <div class="stat-block">Softmax output: 4 labels</div>
        </div>
        <div class="parameter-grid">
            <div class="parameter-item">
                <strong>Input pipeline</strong>
                <span>150×150 RGB</span>
                <p>Images are resized to the CNN’s expected shape and flipped to BGR ordering to mirror the OpenCV reader used during training.</p>
            </div>
            <div class="parameter-item">
                <strong>Convolutional backbone</strong>
                <span>32→64→128 filters, 3×3 kernels</span>
                <p>Five Conv2D blocks with ReLU activations and intermittent MaxPooling distill spatial hierarchies while Dropout(0.3) guards against overfitting.</p>
            </div>
            <div class="parameter-item">
                <strong>Classifier head</strong>
                <span>Flatten → Dense(512)×2 → Softmax</span>
                <p>Two dense layers (512 units each) encode features before a softmax layer maps to the four tumor classes.</p>
            </div>
            <div class="parameter-item">
                <strong>Training config</strong>
                <span>Adam + categorical crossentropy</span>
                <p>Trained for 20 epochs with 10% validation split; Adam’s adaptive learning (lr≈1e-3) minimizes cross-entropy loss.</p>
            </div>
        </div>
        <div>
            <p class="muted">Training data</p>
            <ul class="dataset-list">
                <li>~3,000 MRI scans from the Kaggle Brain Tumor Classification dataset covering glioma, meningioma, pituitary, and no-tumor folders.</li>
                <li>Training/validation split applied after shuffling to keep each class represented in both folds.</li>
            </ul>
        </div>
        <div>
            <p class="muted">Performance tracking</p>
            <ul class="metric-list">
                <li>Accuracy &amp; loss logged per epoch via <code>history.history['accuracy']</code> and <code>history.history['val_accuracy']</code>.</li>
                <li>Dropout, pooling, and data augmentation from resizing keep the model generalized across scanner variations.</li>
            </ul>
        </div>
    </article>

    <article class="model-card">
        <div>
            <span class="badge">Logistic</span>
            <h3>Heart Disease Regression</h3>
            <p class="muted">Predicts coronary disease probability from 13 clinical measurements stored in <code>heart.csv</code>. Stratified splitting keeps class balance.</p>
        </div>
        <div class="stat-row">
            <div class="stat-block">Split: 80% train / 20% test</div>
            <div class="stat-block">Target: 0 = healthy, 1 = defective</div>
        </div>
        <div class="parameter-grid">
            <div class="parameter-item">
                <strong>Feature set</strong>
                <span>age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpeak, slope, ca, thal</span>
                <p>These continuous and categorical variables capture vitals, ECG interpretations, and angiographic scores.</p>
            </div>
            <div class="parameter-item">
                <strong>Model config</strong>
                <span>C=1.0, penalty=L2, solver=lbfgs</span>
                <p>Regularized logistic regression balances bias/variance over the stratified training fold.</p>
            </div>
            <div class="parameter-item">
                <strong>Training split</strong>
                <span>Stratified (random_state=2)</span>
                <p>Stratification ensures healthy and defective hearts remain proportionally represented.</p>
            </div>
            <div class="parameter-item">
                <strong>Prediction rule</strong>
                <span>Probability → threshold 0.5</span>
                <p>The stored logistic model evaluates <code>predict_proba()</code> and flags high-risk when the disease probability exceeds 0.5.</p>
            </div>
        </div>
        <div>
            <p class="muted">Training data</p>
            <ul class="dataset-list">
                <li>The Cleveland Heart Disease dataset (<code>heart.csv</code>) with 303 rows of ECG and angiography measurements.</li>
                <li>Missing values were inspected manually, and the complete rows were used for training.</li>
            </ul>
        </div>
        <div>
            <p class="muted">Performance tracking</p>
            <ul class="metric-list">
                <li>Accuracy measured on both train and test partitions via <code>accuracy_score</code>.</li>
                <li>An <code>InconsistentVersionWarning</code> is raised when loading the saved scikit-learn 1.0.2 estimator with v1.6.1, but the parameters remain compatible.</li>
            </ul>
        </div>
    </article>

    <article class="model-card">
        <div>
            <span class="badge">Random Forest</span>
            <h3>Diabetes Ensemble</h3>
            <p class="muted">Predicts diabetes onset from the Pima Indians dataset using engineered categories and a RandomForestClassifier.</p>
        </div>
        <div class="stat-row">
            <div class="stat-block">Features: 8 core measures + categorical encodings</div>
            <div class="stat-block">Split: 80% train / 20% test</div>
        </div>
        <div class="parameter-grid">
            <div class="parameter-item">
                <strong>Feature engineering</strong>
                <span>Derived BMI, insulin, glucose bins</span>
                <p>Zeros replaced with medians and Local Outlier Factor pruned outliers before creating NewBMI, NewInsulinScore, and NewGlucose buckets.</p>
            </div>
            <div class="parameter-item">
                <strong>Scaling</strong>
                <span>RobustScaler + StandardScaler</span>
                <p>RobustScaler first handles skew, then StandardScaler zeroes the mean and scales unit variance for each predictor.</p>
            </div>
            <div class="parameter-item">
                <strong>Random Forest params</strong>
                <span>n_estimators=100, criterion=gini, max_features=sqrt, random_state=42</span>
                <p>The ensemble averages 100 trees, limiting splits to √features to decorrelate decisions.</p>
            </div>
            <div class="parameter-item">
                <strong>Evaluation</strong>
                <span>Accuracy + ROC comparison</span>
                <p>Seven classifiers (LR, SVM, KNN, DT, RF, GBC, XGBoost) were benchmarked; RandomForest was picked for persistence.</p>
            </div>
        </div>
        <div>
            <p class="muted">Training data</p>
            <ul class="dataset-list">
                <li><code>diabetes.csv</code> (Pima Indians Diabetes Database) with pregnancies, glucose, blood pressure, BMI, pedigree factors, and outcomes.</li>
                <li>LOF, imputation, and categorical injection encode risk zones before passing examples to the ensemble.</li>
            </ul>
        </div>
        <div>
            <p class="muted">Performance tracking</p>
            <ul class="metric-list">
                <li>Accuracy and ROC curves visualized for every candidate algorithm to justify the final choice.</li>
                <li>The Flask app now loads the persisted <code>RandomForestClassifier(random_state=42)</code>.</li>
            </ul>
        </div>
    </article>
</section>
{% endblock %}
